{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import os\nimport sys\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\n\nimport tensorflow as tf\nfrom tensorflow import keras\ntf.keras.backend.set_floatx('float32') #why?",
      "metadata": {},
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "curdir = os.getcwd()\nprevdir = os.path.dirname(curdir)\n\nfolder = prevdir + '/datasets/ml-100k/'\nrating_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\nratings = pd.read_csv(folder + 'u.data', sep = '\\t', names = rating_cols, encoding = 'latin-1')\nratings.drop('timestamp', axis = 1, inplace = True)",
      "metadata": {},
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "min_rated = 3\nvalue_counts = ratings['movie_id'].value_counts()\nremove_movies = value_counts.loc[value_counts < min_rated].index\nratings = ratings.loc[~ ratings['movie_id'].isin(remove_movies)]",
      "metadata": {},
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Helpful article on data splitting: https://stackoverflow.com/questions/43129764/splitting-data-set-into-training-and-testing-sets-on-recommender-systems",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Preparing the dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\ndef split_dataset(ratings):\n\n    users = set(ratings['user_id'].unique())\n    movie_counts = ratings['movie_id'].value_counts()\n    ratings = ratings.sort_values(by = 'movie_id').reset_index(drop = True)\n    rem_ratings = ratings.copy()\n\n    idx = 0\n    count = 0\n    X_test = None\n    X_train = None\n    X_valid = None\n    while idx < ratings.shape[0]:\n        \n        print(f'\\rprocessing {idx} of {ratings.shape[0] - 1}', end = \" \", sep = \" \")\n        \n        if count == 0:\n            if not isinstance(X_train, pd.DataFrame):\n                first_row = ratings.loc[idx].to_dict()\n                X_train = pd.DataFrame(first_row, index = [0])     \n            else:\n                X_train = X_train.append(ratings.loc[idx], ignore_index = True)\n            rem_ratings.drop(idx, inplace = True)\n\n            idx += 1\n            count = 1\n               \n        elif count == 1:\n            if not isinstance(X_valid, pd.DataFrame):\n                first_row = ratings.loc[idx].to_dict()\n                X_valid = pd.DataFrame(first_row, index = [0])\n            else:\n                X_valid = X_valid.append(ratings.loc[idx], ignore_index = True)\n            rem_ratings.drop(idx, inplace = True)\n            \n            idx += 1\n            count = 2\n            \n        elif count == 2:\n            if not isinstance(X_test, pd.DataFrame):\n                first_row = ratings.loc[idx].to_dict()\n                X_test = pd.DataFrame(first_row, index = [0])\n            else:\n                X_test = X_test.append(ratings.loc[idx], ignore_index = True)\n            \n            count = 0\n            rem_ratings.drop(idx, inplace = True)\n            movie_id = ratings.loc[idx, 'movie_id'] \n            assert movie_counts[movie_id] > 2\n            idx += (movie_counts[movie_id] - 2)\n    \n    train, test= train_test_split(rem_ratings, \n                                  test_size = 0.2, \n                                  random_state = 42)\n    \n    X_test = pd.concat([X_test, test])\n    X_train = pd.concat([X_train, train])\n    \n    users_test = X_test['user_id'].unique()\n    users_valid = X_valid['user_id'].unique()\n    users_train = X_train['user_id'].unique()\n    \n    rem_users_test = list(users.difference(set(users_test)))\n    rem_users_valid = list(users.difference(set(users_valid)))\n    rem_users_train = list(users.difference(set(users_train)))\n\n    for user_id in rem_users_test:\n        row = ratings.loc[ratings['user_id'] == user_id].sample()\n        X_test = X_test.append(row, ignore_index = True)\n        \n    for user_id in rem_users_valid:\n        row = ratings.loc[ratings['user_id'] == user_id].sample()\n        X_valid = X_valid.append(row, ignore_index = True)\n        \n    for user_id in rem_users_train:\n        row = ratings.loc[ratings['user_id'] == user_id].sample()\n        X_train = X_train.append(row, ignore_index = True)\n    \n    return X_train, X_valid, X_test\n\nX_train, X_valid, X_test = split_dataset(ratings)",
      "metadata": {},
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "processing 99721 of 99722 "
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "matrix = X_train.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\nmatrix_valid = X_valid.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()",
      "metadata": {},
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef score(cf_model):\n    id_pairs = zip(X_test['user_id'], X_test['movie_id'])\n    y_pred = np.array([cf_model(user, movie) for (user, movie) in id_pairs])\n    y_true = np.array(X_test['rating'])\n    return rmse(y_true, y_pred)",
      "metadata": {},
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def print_statusbar(iteration, total, losses_list, metrics=None):\n    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in losses_list + (metrics or [])])\n    end = \"\" if iteration < total else \"\\n\"\n    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end = end)",
      "metadata": {},
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def random_batch(matrix, filled_idx_tuples, batch_size = None):\n    \n    user_batch = None\n    item_batch = None\n    ratings_batch = None\n    \n    if batch_size == None:\n        indices = range(len(filled_idx_tuples))\n    else:\n        indices = np.random.randint(len(filled_idx_tuples), size = batch_size)\n    \n    for i, rdm_idx in enumerate(indices):\n        (user_idx, item_idx) = filled_idx_tuples[rdm_idx]\n\n        if i == 0:\n            user_batch = matrix[user_idx, :]\n            item_batch = matrix[:, item_idx]\n            ratings_batch = matrix[user_idx, item_idx]\n\n            # user_batch = tf.convert_to_tensor(matrix[user_idx, :])\n            # item_batch = tf.convert_to_tensor(matrix[:, item_idx])\n            # ratings_batch = tf.convert_to_tensor(matrix[user_idx, item_idx])\n\n            # user_batch = tf.expand_dims(matrix[user_idx, :], axis = 0)\n            # item_batch = tf.expand_dims(matrix[:, item_idx], axis = 0)\n            # ratings_batch = tf.expand_dims(matrix[user_idx, item_idx], axis = 0)\n \n        else:        \n            user_batch = np.vstack((user_batch, matrix[user_idx, :]))\n            item_batch = np.vstack((item_batch, matrix[:, item_idx]))\n            ratings_batch = np.vstack((ratings_batch, matrix[user_idx, item_idx]))\n            \n            # user_batch = tf.concat([user_batch, tf.expand_dims(matrix[user_idx, :], axis = 0)], axis = 0)\n            # item_batch = tf.concat([item_batch, tf.expand_dims(matrix[:, item_idx], axis = 0)], axis = 0)\n            # ratings_batch = tf.concat([ratings_batch, tf.expand_dims(matrix[user_idx, item_idx], axis = 0)], axis = 0)\n\n    return ((user_batch, item_batch), ratings_batch)",
      "metadata": {},
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "idx_valid = np.where(~ np.isnan(matrix_valid))\nfilled_idx_tuples_valid = list(zip(*idx_valid))\nvalidation_set = random_batch(matrix_valid, filled_idx_tuples_valid)",
      "metadata": {
        "tags": []
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class GMFLayer(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        \n    def build(self, input_shape):\n        assert(input_shape[0][1] == input_shape[1][1]) \n        \n    def call(self, inputs):\n        user_vec, item_vec = inputs\n        elem_product = tf.math.multiply(user_vec, item_vec)\n        return elem_product \n    \n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape[0]",
      "metadata": {},
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "@tf.function\ndef sigmoid_1_5(z):\n    return 1 + (4 * tf.math.sigmoid(z))",
      "metadata": {},
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.layers import Dense, LeakyReLU\n\nclass DenseBlock(keras.layers.Layer):\n    def __init__(self, n_layers, n_units, regularizer = None, leaky_alpha = 0.3, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = []\n        for __ in range(n_layers):\n            self.hidden.append(Dense(n_units, kernel_regularizer = regularizer))\n            self.hidden.append(LeakyReLU(alpha = leaky_alpha))\n        \n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return Z\n    ",
      "metadata": {},
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.layers import Dense, LeakyReLU\n\nclass NeuMFModel(keras.models.Model):\n    def __init__(self, n_dense_block = 4, n_units = 30, regul = None, leaky_apha = 0.3, **kwargs):\n        \n        super().__init__(**kwargs)\n        self.n_units = n_units\n        \n        self.mf_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_user_vec') #\n        self.leaky_mf_user_vec = LeakyReLU(alpha = leaky_alpha)\n        \n        self.mf_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_item_vec') #\n        self.leaky_mf_item_vec = LeakyReLU(alpha = leaky_alpha)\n        \n        self.mlp_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_user_vec') #\n        self.leaky_mlp_user_vec = LeakyReLU(alpha = leaky_alpha)\n        \n        self.mlp_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_item_vec') #\n        self.leaky_mlp_item_vec = LeakyReLU(alpha = leaky_alpha)\n\n        self.concat_mlp_layer = keras.layers.Concatenate(name = 'concat_mlp_layer')\n\n        self.gmf_layer = GMFLayer(name = 'gmf_layer')\n        self.mlp_block = DenseBlock(n_dense_block, n_units, regularizer = regul, name = 'mlp_block')\n        \n        self.neu_mf_layer = keras.layers.Concatenate(name = 'neu_mf_layer')\n        self.rating_output = Dense(1, activation = sigmoid_1_5, \n                                   kernel_regularizer = regul, name = 'output')\n        \n    def call(self, inputs):\n        user_input, item_input = inputs        \n        mf_user_vec = self.mf_user_vec(user_input)\n        mf_user_vec_act = self.leaky_mf_user_vec(mf_user_vec)\n        \n        mf_item_vec = self.mf_item_vec(item_input)\n        mf_item_vec_act = self.leaky_mf_item_vec(mf_item_vec)\n        \n        mlp_user_vec = self.mlp_user_vec(user_input)\n        mlp_user_vec_act = self.leaky_mlp_user_vec(mlp_user_vec)\n        \n        mlp_item_vec = self.mlp_item_vec(item_input)\n        mlp_item_vec_act = self.leaky_mlp_item_vec(mlp_item_vec)\n        \n        concat_mlp_layer = self.concat_mlp_layer([mlp_user_vec_act, mlp_item_vec_act])\n        mlp_block = self.mlp_block(concat_mlp_layer)\n        gmf_layer = self.gmf_layer([mf_user_vec_act, mf_item_vec_act])\n        neu_mf_layer = self.neu_mf_layer([gmf_layer, mlp_block])\n        rating_output = self.rating_output(neu_mf_layer)\n        return rating_output",
      "metadata": {},
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def training_loop(model, matrix, validation_set, optimizer, n_epochs, batch_size = 64):\n    \n    indices = np.where(~ np.isnan(matrix))\n    filled_idx_tuples = list(zip(*indices))\n    matrix = np.nan_to_num(matrix)\n    \n    X_valid, y_valid = validation_set\n    user_valid_set, movie_valid_set = X_valid\n    user_valid_set = np.nan_to_num(user_valid_set)\n    movie_valid_set = np.nan_to_num(movie_valid_set)\n    \n    n_steps = len(filled_idx_tuples) // batch_size\n    loss_func = keras.losses.MeanSquaredError()\n    metrics = [keras.metrics.MeanSquaredError(name = 'validation mse')]\n    mean_loss = keras.metrics.Mean(name = 'train mse')\n    mean_total_loss = keras.metrics.Mean(name = 'train mse + reg')\n        \n    for epoch in range(1, n_epochs + 1):\n        print(\"Epoch {}/{}\".format(epoch, n_epochs))\n        for step in range(1, n_steps + 1):\n            input_batch, output_batch = random_batch(matrix, filled_idx_tuples, batch_size)\n            with tf.GradientTape() as tape:\n                y_pred = model(input_batch)\n                loss = tf.reduce_mean(loss_func(output_batch, y_pred))\n                total_loss = tf.add_n([loss] + model.losses)\n    \n            gradients = tape.gradient(total_loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            \n            mean_loss(loss)\n            mean_total_loss(total_loss)\n            \n            y_pred_valid = model((user_valid_set, movie_valid_set))\n            for idx, metric in enumerate(metrics):\n                if idx < len(metrics) - 1:\n                    metric(output_batch, y_pred)\n                else: \n                    metric(y_valid, y_pred_valid) \n            \n            print_statusbar(step * batch_size, len(filled_idx_tuples), [mean_loss, mean_total_loss], metrics)\n            \n        print_statusbar(len(filled_idx_tuples), len(filled_idx_tuples), [mean_loss, mean_total_loss], metrics)\n        for metric in [mean_loss, mean_total_loss] + metrics:\n            metric.reset_states()\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "leaky_alpha = 0.4\nreg = keras.regularizers.l2(0.5)\nopt = keras.optimizers.Adam(1e-5)\n\nmodel = NeuMFModel(name = 'neu_mf_model', n_dense_block = 5, n_units = 30, \n                   regul = reg, leaky_apha = leaky_alpha)\n\ntraining_loop(model, matrix, validation_set, batch_size = 256, optimizer = opt, n_epochs = 10)",
      "metadata": {},
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/10\n\nWARNING:tensorflow:Layer neu_mf_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\n\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\n\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n\n\n77716/77716 - train mse: 1.9993 - train mse + reg: 192.3038 - validation mse: 1.5009\n\nEpoch 2/10\n\n77716/77716 - train mse: 1.6724 - train mse + reg: 176.8075 - validation mse: 1.5023\n\nEpoch 3/10\n\n77716/77716 - train mse: 1.5339 - train mse + reg: 162.8301 - validation mse: 1.5033\n\nEpoch 4/10\n\n77716/77716 - train mse: 1.4026 - train mse + reg: 150.0879 - validation mse: 1.5029\n\nEpoch 5/10\n\n77716/77716 - train mse: 1.3340 - train mse + reg: 138.5123 - validation mse: 1.5021\n\nEpoch 6/10\n\n38144/77716 - train mse: 1.2989 - train mse + reg: 130.5204 - validation mse: 1.5013"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-26-d5e6b61eb982>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                    regul = reg, leaky_apha = leaky_alpha)\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-25-8e3d15eeba6a>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(model, matrix, validation_set, optimizer, n_epochs, batch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch {}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0minput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilled_idx_tuples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-d501a5b57d20>\u001b[0m in \u001b[0;36mrandom_batch\u001b[1;34m(matrix, filled_idx_tuples, batch_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0muser_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mitem_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mratings_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Debugging the model",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Try to overfit using a small dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "users = ratings['user_id'].unique()\nmovies = ratings['movie_id'].unique()\n\nperc = 0.001\nusers_sub = np.random.choice(users, int(len(users) * perc), replace = False)\nmovies_sub = np.random.choice(movies, int(len(movies) * perc), replace = False)\n\nfilter_ = (ratings['user_id'].isin(users_sub)) | (ratings['movie_id'].isin(movies_sub)) \nratings_sub = ratings.loc[filter_]\n\nmin_rated = 3\nvalue_counts = ratings_sub['movie_id'].value_counts()\nremove_movies = value_counts.loc[value_counts < min_rated].index\nratings_sub = ratings_sub.loc[~ ratings_sub['movie_id'].isin(remove_movies)]",
      "metadata": {},
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X_train_sub, X_valid_sub, X_test_sub = split_dataset(ratings_sub)",
      "metadata": {},
      "execution_count": 134,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n0 of 190 \n1 of 190 \n2 of 190 "
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "matrix_sub = X_train_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\nmatrix_valid_sub = X_valid_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()",
      "metadata": {},
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "idx_valid_sub = np.where(~ np.isnan(matrix_valid_sub))\nfilled_idx_tuples_valid_sub = list(zip(*idx_valid_sub))\nvalidation_set_sub = random_batch(matrix_valid_sub, filled_idx_tuples_valid_sub)",
      "metadata": {},
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "leaky_alpha = 0.4\nreg = keras.regularizers.l2(0.1)\nopt = keras.optimizers.Adam(0.0001)\n\nmodel = NeuMFModel(name = 'neu_mf_model', n_dense_block = 5, n_units = 30, \n                   regul = reg, leaky_apha = leaky_alpha)\n\ntraining_loop(model, matrix_sub, validation_set_sub, optimizer = opt, n_epochs = 1020)\n\n# Result: we are able to overfit a small dataset",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Check loss from random inputs ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n_users = 250\nn_movies = 300\ntrain_perc_nan = 0.99\nvalid_perc_nan = 0.999\n\nmatrix_rdm = 5 * np.random.rand(n_users, n_movies)\nmatrix_rdm.ravel()[(\n    np.random.choice(matrix_rdm.size, int(n_movies * n_users * train_perc_nan), replace = False))] = np.nan\n\nmatrix_valid_rdm = 5 * np.random.rand(n_users, n_movies)\nmatrix_valid_rdm.ravel()[(\n    np.random.choice(matrix_valid_rdm.size, int(n_movies * n_users * valid_perc_nan), replace = False))] = np.nan",
      "metadata": {},
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "idx_valid_rdm = np.where(~ np.isnan(matrix_valid_rdm))\nfilled_idx_tuples_valid_rdm = list(zip(*idx_valid_rdm))\nvalidation_set_rdm = random_batch(matrix_valid_rdm, filled_idx_tuples_valid_rdm)",
      "metadata": {},
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "leaky_alpha = 0.4\nreg = keras.regularizers.l2(0.1)\nopt = keras.optimizers.Adam(0.0001)\n\nmodel = NeuMFModel(name = 'neu_mf_model', n_dense_block = 5, n_units = 30, \n                   regul = reg, leaky_apha = leaky_alpha)\n\ntraining_loop(model, matrix_rdm, validation_set_rdm, optimizer = opt, n_epochs = 20)\n# Result: dataset is informative since loss of random noise is high",
      "metadata": {},
      "execution_count": 146,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/20\n\n750/750 - train mse: 2.3224 - train mse + reg: 39.6613 - validation mse: 2.2497\n\nEpoch 2/20\n\n750/750 - train mse: 2.5132 - train mse + reg: 39.1978 - validation mse: 2.2424\n\nEpoch 3/20\n\n750/750 - train mse: 2.3061 - train mse + reg: 38.3481 - validation mse: 2.2350\n\nEpoch 4/20\n\n750/750 - train mse: 2.3594 - train mse + reg: 37.7692 - validation mse: 2.2285\n\nEpoch 5/20\n\n750/750 - train mse: 2.3485 - train mse + reg: 37.1371 - validation mse: 2.2219\n\nEpoch 6/20\n\n750/750 - train mse: 2.2244 - train mse + reg: 36.4030 - validation mse: 2.2152\n\nEpoch 7/20\n\n750/750 - train mse: 2.3235 - train mse + reg: 35.9028 - validation mse: 2.2085\n\nEpoch 8/20\n\n750/750 - train mse: 2.3679 - train mse + reg: 35.3589 - validation mse: 2.2014\n\nEpoch 9/20\n\n750/750 - train mse: 2.2578 - train mse + reg: 34.6712 - validation mse: 2.1945\n\nEpoch 10/20\n\n750/750 - train mse: 2.1991 - train mse + reg: 34.0455 - validation mse: 2.1876\n\nEpoch 11/20\n\n750/750 - train mse: 2.3279 - train mse + reg: 33.6170 - validation mse: 2.1814\n\nEpoch 12/20\n\n750/750 - train mse: 2.2234 - train mse + reg: 32.9652 - validation mse: 2.1752\n\nEpoch 13/20\n\n750/750 - train mse: 2.1770 - train mse + reg: 32.3814 - validation mse: 2.1693\n\nEpoch 14/20\n\n750/750 - train mse: 2.3512 - train mse + reg: 32.0281 - validation mse: 2.1633\n\nEpoch 15/20\n\n750/750 - train mse: 2.1387 - train mse + reg: 31.2977 - validation mse: 2.1575\n\nEpoch 16/20\n\n750/750 - train mse: 2.2043 - train mse + reg: 30.8542 - validation mse: 2.1521\n\nEpoch 17/20\n\n750/750 - train mse: 2.2906 - train mse + reg: 30.4408 - validation mse: 2.1465\n\nEpoch 18/20\n\n750/750 - train mse: 2.2361 - train mse + reg: 29.8956 - validation mse: 2.1405\n\nEpoch 19/20\n\n750/750 - train mse: 2.2220 - train mse + reg: 29.3994 - validation mse: 2.1354\n\nEpoch 20/20\n\n750/750 - train mse: 2.1717 - train mse + reg: 28.8758 - validation mse: 2.1304\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Try simpler NN models",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Weighted combination of element-wise product",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "users = ratings['user_id'].unique()\nmovies = ratings['movie_id'].unique()\n\nperc = 1\nusers_sub = np.random.choice(users, int(len(users) * perc), replace = False)\nmovies_sub = np.random.choice(movies, int(len(movies) * perc), replace = False)\n\nfilter_ = (ratings['user_id'].isin(users_sub)) | (ratings['movie_id'].isin(movies_sub)) \nratings_sub = ratings.loc[filter_]\n\nmin_rated = 3\nvalue_counts = ratings_sub['movie_id'].value_counts()\nremove_movies = value_counts.loc[value_counts < min_rated].index\nratings_sub = ratings_sub.loc[~ ratings_sub['movie_id'].isin(remove_movies)]",
      "metadata": {},
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X_train_sub, X_valid_sub, X_test_sub = split_dataset(ratings_sub)\n\nmatrix_sub = X_train_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\nmatrix_valid_sub = X_valid_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n\nidx_valid_sub = np.where(~ np.isnan(matrix_valid_sub))\nfilled_idx_tuples_valid_sub = list(zip(*idx_valid_sub))\nvalidation_set_sub = random_batch(matrix_valid_sub, filled_idx_tuples_valid_sub)",
      "metadata": {},
      "execution_count": 153,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "99721 of 99722                                                                                                                                                             "
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.layers import Dense, LeakyReLU\n\nclass NeuCFModel(keras.models.Model):\n    def __init__(self, n_dense_block = 4, n_units = 30, regul = None, leaky_apha = 0.3, **kwargs):\n        \n        super().__init__(**kwargs)\n        self.n_units = n_units\n        \n        self.mf_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_user_vec') #\n        self.leaky_mf_user_vec = LeakyReLU(alpha = leaky_alpha)\n        \n        self.mf_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_item_vec') #\n        self.leaky_mf_item_vec = LeakyReLU(alpha = leaky_alpha)\n        \n        self.gmf_layer = GMFLayer(name = 'gmf_layer')\n        self.rating_output = Dense(1, activation = sigmoid_1_5, kernel_regularizer = regul, name = 'output')\n        \n    def call(self, inputs):\n        user_input, item_input = inputs        \n        mf_user_vec = self.mf_user_vec(user_input)\n        mf_user_vec_act = self.leaky_mf_user_vec(mf_user_vec)\n        \n        mf_item_vec = self.mf_item_vec(item_input)\n        mf_item_vec_act = self.leaky_mf_item_vec(mf_item_vec)\n        \n        gmf_layer = self.gmf_layer([mf_user_vec_act, mf_item_vec_act])\n        rating_output = self.rating_output(gmf_layer)\n        return rating_output",
      "metadata": {},
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "leaky_alpha = 0.4\nreg = keras.regularizers.l2(0.1)\nopt = keras.optimizers.Adam(0.0001)\n\nmodel = NeuCFModel(name = 'neu_cf_model', n_dense_block = 5, n_units = 30, \n                   regul = reg, leaky_apha = leaky_alpha)\n\ntraining_loop(model, matrix_sub, validation_set_sub, optimizer = opt, n_epochs = 20)\n# Result: effective at learning",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Only MLP",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "users = ratings['user_id'].unique()\nmovies = ratings['movie_id'].unique()\n\nperc = 1\nusers_sub = np.random.choice(users, int(len(users) * perc), replace = False)\nmovies_sub = np.random.choice(movies, int(len(movies) * perc), replace = False)\n\nfilter_ = (ratings['user_id'].isin(users_sub)) | (ratings['movie_id'].isin(movies_sub)) \nratings_sub = ratings.loc[filter_]\n\nmin_rated = 3\nvalue_counts = ratings_sub['movie_id'].value_counts()\nremove_movies = value_counts.loc[value_counts < min_rated].index\nratings_sub = ratings_sub.loc[~ ratings_sub['movie_id'].isin(remove_movies)]",
      "metadata": {},
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X_train_sub, X_valid_sub, X_test_sub = split_dataset(ratings_sub)\n\nmatrix_sub = X_train_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\nmatrix_valid_sub = X_valid_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n\nidx_valid_sub = np.where(~ np.isnan(matrix_valid_sub))\nfilled_idx_tuples_valid_sub = list(zip(*idx_valid_sub))\nvalidation_set_sub = random_batch(matrix_valid_sub, filled_idx_tuples_valid_sub)",
      "metadata": {},
      "execution_count": 294,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1814 of 1814       "
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.layers import Dense, LeakyReLU\n\nclass DenseBlock(keras.layers.Layer):\n    def __init__(self, n_layers, n_units, regularizer = None, leaky_alpha = 0.3, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = []\n        for __ in range(n_layers):\n            self.hidden.append(Dense(n_units, kernel_regularizer = regularizer))\n            self.hidden.append(LeakyReLU(alpha = leaky_alpha))\n        \n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return Z\n    ",
      "metadata": {},
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.layers import Dense, LeakyReLU\n\nclass EncoderMLP(keras.models.Model):\n    def __init__(self, n_dense_block = 4, n_units = 30, regul = None, leaky_apha = 0.3, **kwargs):\n        \n        super().__init__(**kwargs)\n        self.n_units = n_units\n        \n        self.mlp_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_user_vec') #\n        self.leaky_mlp_user_vec = LeakyReLU(alpha = leaky_alpha)\n        \n        self.mlp_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_item_vec') #\n        self.leaky_mlp_item_vec = LeakyReLU(alpha = leaky_alpha)\n\n        self.concat_mlp_layer = keras.layers.Concatenate(name = 'concat_mlp_layer')\n\n        self.gmf_layer = GMFLayer(name = 'gmf_layer')\n        self.mlp_block = DenseBlock(n_dense_block, n_units, regularizer = regul, name = 'mlp_block')\n        self.rating_output = Dense(1, activation = sigmoid_1_5, kernel_regularizer = regul, name = 'output')\n        \n    def call(self, inputs):\n        \n        user_input, item_input = inputs        \n        mlp_user_vec = self.mlp_user_vec(user_input)\n        mlp_user_vec_act = self.leaky_mlp_user_vec(mlp_user_vec)\n        \n        mlp_item_vec = self.mlp_item_vec(item_input)\n        mlp_item_vec_act = self.leaky_mlp_item_vec(mlp_item_vec)\n        \n        concat_mlp_layer = self.concat_mlp_layer([mlp_user_vec_act, mlp_item_vec_act])\n        mlp_block = self.mlp_block(concat_mlp_layer)\n        rating_output = self.rating_output(mlp_block)\n        return rating_output",
      "metadata": {},
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "leaky_alpha = 0.4\nreg = keras.regularizers.l2(0.1)\nopt = keras.optimizers.Adam(0.0001)\n\nmodel = NeuCFModel(name = 'enc_model', n_dense_block = 5, n_units = 30, \n                   regul = reg, leaky_apha = leaky_alpha)\n\ntraining_loop(model, matrix_sub, validation_set_sub, optimizer = opt, n_epochs = 20)\n# Result: effective at learning",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}